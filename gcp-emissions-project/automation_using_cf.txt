ETL Cloud Function Automation Guide
This guide will help you set up and deploy an automated ETL Cloud Function on Google Cloud
Platform.
Step 1: Set up a Google Cloud project
-------------------------------------
- Create a new project in Google Cloud Console.
- Enable the necessary APIs: Cloud Functions, Cloud Storage, and BigQuery.
Step 2: Install dependencies
----------------------------
- Install the required Python libraries in your environment:
 google-cloud-storage
 google-cloud-bigquery
 pandas
 numpy
 pyarrow
Step 3: Cloud Function code
----------------------------
Below is the Python code for your ETL Cloud Function:
1. Transform the data by applying business logic.
2. Load the transformed data into Google Cloud Storage and BigQuery.
Code:
```python
import pandas as pd
import numpy as np
from google.cloud import storage, bigquery
import io
RAW_DATA_BUCKET = "etl-raw-data-bkt"
PROCESSED_DATA_BUCKET = "etl-processed-data-bkt"
BQ_PROJECT_ID = "learning-gcp-440109"
BQ_DATASET = "ETL_project"
BQ_TABLE = "ETL_raw_data"
def transform_data(df):
 df['formatted_date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m-%d')
 df['device_type_category'] = np.where(df['device_type'].str.lower().str.contains('phone'), 'Mobile',
'Other')
 df['total_emissions'] = df['media_emissions'] + df['creative_emissions']
 df.drop(['unnecessary_column'], axis=1, inplace=True, errors='ignore')
 return df
def load_to_bigquery(df):
 client = bigquery.Client()
 table_id = f"{BQ_PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}"
 job_config = bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")
 job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
 job.result()
def upload_to_gcs(bucket_name, file_name, df):
 storage_client = storage.Client()
 bucket = storage_client.bucket(bucket_name)
 blob = bucket.blob(file_name)
 blob.upload_from_string(df.to_csv(index=False), content_type="text/csv")
def load_gcs_file(bucket_name, file_name):
 storage_client = storage.Client()
 bucket = storage_client.bucket(bucket_name)
 blob = bucket.blob(file_name)
 data = blob.download_as_text()
 return pd.read_csv(io.StringIO(data))
def process_file(event, context):
 file_name = event['name']
 bucket_name = event['bucket']
 print(f"Processing file: {file_name} from bucket: {bucket_name}")
 raw_df = load_gcs_file(bucket_name, file_name)
 transformed_df = transform_data(raw_df)
 processed_file_name = f"processed_{file_name}"
 upload_to_gcs(PROCESSED_DATA_BUCKET, processed_file_name, transformed_df)
 load_to_bigquery(transformed_df)
 print(f"Data Loaded to BigQuery Table: {BQ_TABLE}")
```
Step 4: Deploy the Cloud Function
----------------------------------
- Deploy the Cloud Function using the following command:
```bash
gcloud functions deploy ETL_cloud_fuction --runtime python312 --trigger-event
google.storage.object.finalize --trigger-resource etl-raw-data-bkt --entry-point process_file 
--region us-central1 --memory 256MB
```
Step 5: Monitor and Test
--------------------------
- Once the function is deployed, test it by uploading a file to the `etl-raw-data-bkt` bucket.
- Monitor the Cloud Function's execution in the Google Cloud Console for logs.
Troubleshooting:
----------------
- If the function is not triggered, ensure that the event trigger is set up correctly.
- If the function fails, check the logs in Cloud Logging to diagnose issues